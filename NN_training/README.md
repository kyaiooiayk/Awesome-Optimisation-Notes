# Optimisers for Neural Nework
***

## PyGAD
- [PyGAD](https://pygad.readthedocs.io/en/latest/) is a genetic algorithm Python 3 library for solving optimization problems. What makes this package interesting with respect to the others is that they cretaed a nice wrapper around both Keras and PyTorch. 
- [Train PyTorch Models Using Genetic Algorithm With PyGAD](https://neptune.ai/blog/train-pytorch-models-using-genetic-algorithm-with-pygad)
- [Genetic Algorithm: Tutorial of PyGAD - Kaggle](https://www.kaggle.com/code/zzettrkalpakbal/genetic-algorithm-tutorial-of-pygad?scriptVersionId=101358507)
***

## Limitation of stachastic gradient methods
- [The "Terpret Problem" and the limits of SGD](https://dselsam.github.io/the-terpret-problem/)
- [sklearn.neural_network.MLPRegressorÂ¶, allows you to use both quasi-Newton and stochastic gradient descent](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)
***

## Available tutorials
- [Scipy optimize wrapper for Keras](https://github.com/kyaiooiayk/Optimisation-Notes/tree/master/NN_training/Scipy_optimize_wrapper_for_keras/GitHub_MD_rendering)
- [Example where a SGD would fail to find the optimal solution](https://github.com/kyaiooiayk/Optimisation-Notes/tree/master/NN_training/Scipy_optimize_wrapper_for_keras/GitHub_MD_rendering)
***
